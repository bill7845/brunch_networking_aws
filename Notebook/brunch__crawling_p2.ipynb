{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "brunch _crawling_p2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyONJy8JAE2NkkH+IqwmB1F5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bill7845/brunch_networking_aws/blob/main/Notebook/brunch__crawling_p2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSSSb4LqSIW2"
      },
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "import requests\n",
        "import json\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "## url pickle load\n",
        "# pickles = ['지구한바퀴_세계여행?q=g','시사·이슈?q=g','IT_트렌드?q=g',\n",
        "#                 '취향저격_영화_리뷰?q=g','오늘은_이런_책?q=g','뮤직_인사이드?q=g',\n",
        "#                  '직장인_현실_조언?q=g','스타트업_경험담?q=g','육아_이야기?q=g',\n",
        "#                  '요리·레시피?q=g','건강·운동?q=g','멘탈_관리_심리_탐구?q=g',\n",
        "#                  '문화·예술?q=g','인문학·철학?q=g','쉽게_읽는_역사?q=g',\n",
        "#                 '우리집_반려동물?q=g','사랑·이별?q=g','감성_에세이?q=g']\n",
        "pickles = ['오늘은_이런_책?q=g']\n",
        "\n",
        "\n",
        "writer_list = []\n",
        "for file in pickles:\n",
        "    with open(file[:-4]+\"_userId.txt\",\"rb\") as fr:\n",
        "        writers = pickle.load(fr)\n",
        "    writer_list.append(writers)  ## [[카테고리1 게시글 url...],[카테고리2 게시글 url], ....[카테고리24 게시글 url]]\n",
        "\n",
        "## 게시글 속 정보 수집\n",
        "def def_craw(writer):\n",
        "    \n",
        "    json_data = {}\n",
        "    data = []\n",
        "    res_text = []\n",
        "    tag_keyword=[]\n",
        "    \n",
        "    tag_title,tag_nickname,tag_publish_date,tag_url,tag_url_plink = None,None,None,None,None\n",
        "    tag_share,tag_like = str,str\n",
        "    for idx,url in enumerate(writer):\n",
        "        if idx % 500 == 0 : print(\"전체\",len(writer),\"중에\",idx)\n",
        "        if res_text == []: # 첫 시작에러 방지\n",
        "            pass\n",
        "        else :\n",
        "            # to json\n",
        "            json_data['title'] = tag_title  \n",
        "            json_data['nickname'] = tag_nickname\n",
        "            json_data['publish_date'] = tag_publish_date\n",
        "            json_data['keyword'] = tmp_keyword   \n",
        "            json_data['like'] = tag_like # like 없는 경우 ''\n",
        "            json_data['share'] = tag_share # share 없는 경우 None            \n",
        "            json_data['comment'] = tag_comment # comment 없는 경우 ''\n",
        "            json_data['url'] = tag_url\n",
        "            json_data['url_plink'] = tag_url_plink \n",
        "            json_data['text'] = res_text\n",
        "\n",
        "        data.append(json_data)\n",
        "        \n",
        "        json_data = {} # 누적방지 초기화\n",
        "        tmp_keyword = [] # 누적방지 초기화\n",
        "        res_text = [] # 누적방지 초기화 \n",
        "        print(url)\n",
        "        # beautifulsoup\n",
        "        html = requests.get('https://brunch.co.kr{text_url}'.format(text_url=url))\n",
        "        soup = BeautifulSoup(html.text, 'html.parser')\n",
        "        \n",
        "        if soup.find('title').text == \"brunch\":\n",
        "            pass\n",
        "        else:\n",
        "            tag_title = soup.find('title').text # 게시글 title\n",
        "            tag_url = soup.find(\"meta\",property='og:url')['content'] # 게시글 본주소\n",
        "            tag_nickname = soup.find(\"meta\",{'name':'article:media_name'})['content'] # 작가 nickname\n",
        "            tag_url_plink = soup.find(\"meta\",property='dg:plink')['content'] # 암호주소? # 모바일?\n",
        "            tag_publish_date = soup.find(\"meta\",property='article:published_time')['content'] # 발행일\n",
        "            tag_keyword = soup.find_all('a',href=re.compile('/keyword')) # 게시글 키워드\n",
        "            tag_like = soup.find('span',{'class':'f_l text_like_count text_default text_with_img_ico ico_likeit_like #like'}) #좋아요 수\n",
        "            tag_share = soup.find('span',{'class':'f_l text_share_count text_default text_with_img_ico'}) # 공유 수\n",
        "            tag_comment = soup.find('span',{'class':'f_l text_comment_count text_default text_with_img_ico'}) # 댓글 수\n",
        "            text_h4 = soup.find_all(class_='wrap_item item_type_text')\n",
        "            \n",
        "            for text in text_h4:\n",
        "                res_text.append(text.text)\n",
        "    \n",
        "            if tag_like == None:\n",
        "                tag_like = \"0\"\n",
        "            else:\n",
        "                tag_like = tag_like.text # 좋아요 수\n",
        "\n",
        "            if tag_share == None:\n",
        "                tag_share == \"0\"\n",
        "            else:\n",
        "                tag_share = tag_share.text # 공유 수\n",
        "\n",
        "            if tag_comment == None:\n",
        "                tag_comment ==\"0\"\n",
        "            else:\n",
        "                tag_comment = tag_comment.text\n",
        "\n",
        "            for keyword in tag_keyword:\n",
        "                tmp_keyword.append(keyword.text)\n",
        "                \n",
        "    return data ## 수집한 정보를 담은 dictionary로 반환\n",
        "\n",
        "categories = ['지구한바퀴_세계여행','시사_이슈','IT_트렌드',\n",
        "                '취항저격_영화리뷰','오늘은_이런책','뮤직_인사이드',\n",
        "                 '직장인_현실조언','스타트업_경험담','육아_이야기',\n",
        "                 '요리_레시피','건강_운동','멘탈관리_심리탐구',\n",
        "                 '문화_예술','인문학_철학','쉽게_읽는_역사',\n",
        "                '우리집_반려동물','사랑_이별','감성_에세이']\n",
        "\n",
        "\n",
        "## 카테고리 -> 게시글의 순서로 2차 크롤링 진행\n",
        "## 카테고리별로 정보를 담은 json 형식으로 저장(총 24개 json file)\n",
        "from collections import OrderedDict\n",
        "for idx,writer in enumerate(writer_list):\n",
        "    to_json = None\n",
        "    data = def_craw(writer) # 2단계 크롤링 실행\n",
        "    \n",
        "    del data[0]\n",
        "    del data[0]\n",
        "    \n",
        "    to_json = OrderedDict()\n",
        "    to_json['name'] = categories[idx] # category name\n",
        "    to_json['version'] = \"2020-06-01\"\n",
        "    to_json['data'] = data\n",
        "    \n",
        "    with open(categories[idx]+\".json\",\"w\") as make_file:\n",
        "        json.dump(to_json,make_file)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}