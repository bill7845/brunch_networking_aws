{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "modeling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "12tg2ewqp6d1yvSLr8E1nsTp1TrRus8mO",
      "authorship_tag": "ABX9TyONG2cga2DqMhyt01NOm4pK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bill7845/brunch_networking_aws/blob/main/Notebook/modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NocVsAkiDpd"
      },
      "source": [
        "\"\"\"\n",
        "colab. konlpy setting\n",
        "\"\"\"\n",
        "!apt-get update\n",
        "!apt-get install g++ openjdk-8-jdk \n",
        "!pip3 install konlpy\n",
        "\n",
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AClBdZKNiIwd"
      },
      "source": [
        "cd Mecab-ko-for-Google-Colab/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ0UKz3siIyx"
      },
      "source": [
        "! bash install_mecab-ko_on_colab190912.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9NJMw9fpKEv"
      },
      "source": [
        "!pip install boto3\n",
        "!pip install awscli"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWChW2QAiI01"
      },
      "source": [
        "## library import\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "import urllib.request\n",
        "import re\n",
        "from ast import literal_eval\n",
        "from google.oauth2 import service_account\n",
        "from google.cloud import storage\n",
        "\n",
        "import boto3\n",
        "import os\n",
        "import io\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "from konlpy.tag import Mecab\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score,f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsAEOZ5rpXgp"
      },
      "source": [
        "\"\"\"\n",
        "aws auth setting\n",
        "\"\"\"\n",
        "!aws configure"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRi_Fx0viI28"
      },
      "source": [
        "\"\"\"\n",
        "bigquery Authentication setting.\n",
        "\"\"\"\n",
        "\n",
        "bq_credential_path = '/content/drive/MyDrive/Colab code/brunch_project/code/gcp_access/brunch-networking-07958d4e3d41.json'\n",
        "credentials = service_account.Credentials.from_service_account_file(bq_credential_path)\n",
        "project_id = 'brunch-networking-303012'\n",
        "\n",
        "def get_data_from_bq(credentials=credentials, project_id=project_id, class_num=int):\n",
        "  \n",
        "  if class_num == None:\n",
        "    \n",
        "    query = \"SELECT * FROM `brunch-networking-303012.brunch_networking.ori_brunch_data`\"\n",
        "    df = pd.read_gbq(query=query, project_id=project_id, credentials=credentials, dialect='standard')\n",
        "    \n",
        "  elif class_num != None:\n",
        "    query = \"SELECT * FROM `brunch-networking-303012.brunch_networking.ori_brunch_data` where class_num = {class_num}\".format(class_num=class_num)\n",
        "    df = pd.read_gbq(query=query, project_id=project_id, credentials=credentials, dialect='standard')\n",
        "\n",
        "  df = df[~df['text'].isnull()] # delete empty text row\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnAUZn4dluOJ"
      },
      "source": [
        "def s3_upload(bucket_name , bucket_key, target_file_path):\n",
        "  \n",
        "  s3 = boto3.client('s3')\n",
        "  \n",
        "  s3.upload_file(\n",
        "      Filename = target_file_path,\n",
        "      Bucket = bucket_name,\n",
        "      Key = bucket_key\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SpJWVXaiI5D"
      },
      "source": [
        "def split_train_test(df):\n",
        "\n",
        "  x_train,x_test,y_train,y_test = train_test_split(df[['text']],df['class_num'],test_size=0.2,random_state=0,stratify=df['class_num'])\n",
        "\n",
        "  return x_train,x_test,y_train,y_test\n",
        "\n",
        "\n",
        "def custom_tagging(df):\n",
        "  mecab = Mecab()\n",
        "\n",
        "  text = df['text']\n",
        "  text_plat = text.values.tolist()\n",
        "\n",
        "  prepared_data = []\n",
        "  tmp = None\n",
        "  for idx in range(len(text_plat)):\n",
        "      tmp = [i[0] for i in mecab.pos(text_plat[idx]) if ( ((i[1]==\"NNG\") or (i[1]==\"NNP\")) and(len(i[0])>1))] # 품사가 명사이면서, 길이가 2이상\n",
        "      prepared_data.append(\" \".join(tmp))\n",
        "\n",
        "  return prepared_data\n",
        "\n",
        "## top_n error\n",
        "## 예측한 최상위 2개 범주 가운데 정답이 없는 경우의 오류율\n",
        "def top_n_error(model, y_test_proba, y_test, top_n=2):\n",
        "\n",
        "  top_n_pred = np.argsort(y_test_proba, axis=1)[:, -top_n:]\n",
        "  class_labels = model.classes_\n",
        "  \n",
        "  true_score = 0\n",
        "  for i in range(len(y_test)):\n",
        "    if y_test.iloc[i] not in class_labels[top_n_pred][i]:\n",
        "      true_score += 1\n",
        "    else :\n",
        "      pass\n",
        "  \n",
        "  error_rate = true_score / len(y_test)\n",
        "\n",
        "  print(\"Top_\"+str(top_n)+\" Error : \",error_rate)\n",
        "\n",
        "  return error_rate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNdirFqhjBZN"
      },
      "source": [
        "# all_df = get_data_from_bq(class_num=None) # load all_df from bigqeury\n",
        "all_df = pd.read_csv(\"/content/drive/MyDrive/Colab code/brunch_project/data/basement/all_df.csv\") # load all_df.csv from google drive\n",
        "\n",
        "x_train, x_test, y_train, y_test = split_train_test(all_df) # split all_df. train/test\n",
        "\n",
        "train_data = custom_tagging(x_train) # mecab pos tagging.\n",
        "test_data = custom_tagging(x_test) # mecab pos taggin.\n",
        "\n",
        "tfidf_vect = TfidfVectorizer(max_df=0.9)\n",
        "\n",
        "tfidf_train_vect = tfidf_vect.fit(train_data) \n",
        "tfidf_train_matrix = tfidf_vect.transform(train_data)\n",
        "path_train = '/content/drive/My Drive/Colab code/brunch_project/data/vect/train/'\n",
        "pickle.dump(tfidf_train_vect, open(os.path.join(path_train,'tfidf_train_vect.pkl'),'wb'),protocol=4) # save google drive\n",
        "pickle.dump(tfidf_train_matrix, open(os.path.join(path_train,'tfidf_train_matrix.pkl'),'wb'),protocol=4) # save google drive\n",
        "\n",
        "# upload s3\n",
        "s3_upload(bucket_name='util-brunch-networking',\n",
        "          bucket_key='train/' + 'tfidf_train_vect.pkl',\n",
        "          target_file_path= path_train + 'tfidf_train_vect.pkl')\n",
        "\n",
        "# tfidf_test_matrix = tfidf_vect.transform(test_data)\n",
        "path_test = '/content/drive/My Drive/Colab code/brunch_project/data/vect/test/'\n",
        "pickle.dump(tfidf_test_matrix, open(os.path.join(path_test,'tfidf_test_matrix.pkl'),'wb'),protocol=4) # save google drive\n",
        "\n",
        "# upload s3\n",
        "s3_upload(bucket_name='util-brunch-networking',\n",
        "          bucket_key='test/' + 'tfidf_test_matrix.pkl',\n",
        "          target_file_path= path_test + 'tfidf_test_matrix.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hHlUiy8iI7Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47922c2e-d181-4cce-8360-7d6de33bfda9"
      },
      "source": [
        "# ## logistic Regression\n",
        "lg_model = LogisticRegression(C=1,multi_class='multinomial',solver='lbfgs')\n",
        "lg_model.fit(tfidf_train_matrix, y_train)\n",
        "\n",
        "model_path = '/content/drive/My Drive/Colab code/brunch_project/data/model_weight/'\n",
        "pickle.dump(lg_model, open(os.path.join(model_path,'classifier_lg.pkl'),'wb'),protocol=4) # save google drive\n",
        "\n",
        "# load model weight. \n",
        "# with open('/content/drive/My Drive/Colab code/brunch_project/data/model_weight/classifier_lg.pkl', 'rb') as f:\n",
        "#   lg_model = pickle.load(f)\n",
        "\n",
        "pred_logistic = lg_model.predict(tfidf_test_matrix) # prediction\n",
        "pred_logistic_proba = lg_model.predict_proba(tfidf_test_matrix) # get prediction probability\n",
        "\n",
        "## check classfication result\n",
        "print(\"classification report\", classification_report(y_test,pred_logistic))\n",
        "print(\"accuracy : \",accuracy_score(y_test,pred_logistic)) \n",
        "print(\"f1_score : \",f1_score(y_test,pred_logistic, average='macro'))\n",
        "top_2_error = top_n_error(lg_model ,pred_logistic_proba, y_test, top_n=2) \n",
        "\n",
        "# upload s3\n",
        "s3_upload(bucket_name='util-brunch-networking',\n",
        "          bucket_key='model_weight/' + 'classifier_lg.pkl',\n",
        "          target_file_path= model_path + 'classifier_lg.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "classification report               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.72      0.73      3373\n",
            "           1       0.74      0.71      0.73       726\n",
            "           2       0.81      0.73      0.77      1652\n",
            "           3       0.64      0.42      0.50      1044\n",
            "           4       0.75      0.62      0.68      1409\n",
            "           5       0.69      0.47      0.56       672\n",
            "           6       0.74      0.69      0.71       792\n",
            "           7       0.80      0.69      0.74      1254\n",
            "           8       0.66      0.54      0.59      4121\n",
            "           9       0.53      0.70      0.60      9513\n",
            "          10       0.74      0.71      0.72      1690\n",
            "          11       0.73      0.66      0.69      1151\n",
            "          12       0.79      0.78      0.78      1576\n",
            "          13       0.80      0.82      0.81      3018\n",
            "          14       0.64      0.56      0.60      2695\n",
            "          15       0.74      0.68      0.71      1249\n",
            "          16       0.75      0.69      0.72      3598\n",
            "          17       0.76      0.66      0.71      1418\n",
            "\n",
            "    accuracy                           0.67     40951\n",
            "   macro avg       0.73      0.66      0.69     40951\n",
            "weighted avg       0.69      0.67      0.67     40951\n",
            "\n",
            "accuracy :  0.6725354692193108\n",
            "f1_score :  0.6874026828635968\n",
            "Top_2 Error :  0.1428780737955117\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}